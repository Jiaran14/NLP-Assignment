{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project: news article\n",
    "\n",
    "A dataset consists of news articles that cover four separate topics: World, Business, Sports, and Sci/Tech are provided. \n",
    "The following code will predict the topic, generate text and then test the top performing NLP classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "train = load_doc('train.csv')\n",
    "test = load_doc('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare y\n",
    "import pandas as pd\n",
    "traindata = pd.read_csv('train.csv', names=['label','title','text'])\n",
    "testdata = pd.read_csv('test.csv', names=['label','title','text'])\n",
    "\n",
    "ytrain = traindata['label']\n",
    "ytest = testdata['label']\n",
    "y_train = ytrain.values\n",
    "y_test = ytest.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 3, ..., 2, 3, 4])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TF-IDF with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean doc\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re.sub(r\"[{}]+\".format(punctuation),\" \",w) for w in tokens]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 2]\n",
    "    # filter out numbers\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    #lower tokens\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare training and test dataset\n",
    "def process_docs(doc):\n",
    "    documents = list()\n",
    "    doc2 = doc.split('\\n')\n",
    "    for line in doc2:\n",
    "        line = clean_doc(line)\n",
    "        documents.append(line)\n",
    "    documents.pop()\n",
    "    return documents\n",
    "\n",
    "X_train = process_docs(train)\n",
    "X_test = process_docs(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['guard blake out week washington wizards point guard steve blake miss first month season injuring ankle pickup',\n",
       " 'acquires synstar move designed help better compete said acquired synstar technology services',\n",
       " 'airways pilots vote salary airways pilots voted approve new labor agreement yesterday reduce salaries percent save airline million',\n",
       " 'risks panel examining radiation risks says official estimates dangers health may wide mark',\n",
       " 'fate line partisans sides calling polarized important election presidential recall referendum today determine course democracy could buffet world oil both campaigns also utterly convinced']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(X_train)\n",
    "text_vector=tfidf.transform(X_train).toarray()\n",
    "train_text_vector=tfidf.transform(X_train)\n",
    "test_text_vector=tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB running time: 0.02191925048828125\n"
     ]
    }
   ],
   "source": [
    "# fit NB model and calculate running time\n",
    "start = time.time()\n",
    "clf = MultinomialNB()\n",
    "clf = clf.fit(train_text_vector,y_train)\n",
    "end = time.time()\n",
    "NB_time = end-start\n",
    "print('NB running time:', NB_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB-tfidf accuracy:  0.88625\n"
     ]
    }
   ],
   "source": [
    "#make presiction and calculate accuracy\n",
    "prediction=clf.predict(test_text_vector)\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "print('NB-tfidf accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word2vec with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from collections import Counter\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from keras.utils import plot_model\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc2(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re.sub(r\"[{}]+\".format(punctuation),\" \",w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45724\n",
      "[('said', 5567), ('The', 4549), ('new', 3370), ('US', 2897), ('first', 2730), ('two', 2491), ('last', 1998), ('company', 1860), ('one', 1823), ('New', 1754)]\n"
     ]
    }
   ],
   "source": [
    "#apply\n",
    "tokens = clean_doc2(train)\n",
    "vocab = Counter()\n",
    "vocab.update(tokens)\n",
    "print(len(vocab))\n",
    "print(vocab.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28474\n"
     ]
    }
   ],
   "source": [
    "# keep tokens with a min occurrence\n",
    "min_occurrence = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurrence]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    "\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Encode and Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn doc into clean tokens\n",
    "def clean_doc3(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each word\n",
    "    [re.sub(r\"[{}]+\".format(punctuation),\" \",w) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare training and test dataset\n",
    "def process_docs(doc,vocab):\n",
    "    documents = list()\n",
    "    doc2 = doc.split('\\n')\n",
    "    for line in doc2:\n",
    "        line = clean_doc3(line, vocab)\n",
    "        documents.append(line)\n",
    "    documents.pop()\n",
    "    return documents\n",
    "X_train2 = process_docs(train,vocab)\n",
    "X_test2 = process_docs(test,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length 71\n"
     ]
    }
   ],
   "source": [
    "# calculate the maximum sequence length\n",
    "max_length = max([len(s.split()) for s in X_train2])\n",
    "print('max_length', max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad X and one-hot y\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train2)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(X_train2)\n",
    "word_index = tokenizer.word_index\n",
    "Xtrain = pad_sequences(sequences, maxlen=max_length)\n",
    "labels_train= to_categorical(np.asarray(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 22540\n"
     ]
    }
   ],
   "source": [
    "# define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size:', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define word2vec model to embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence= []\n",
    "labels = []\n",
    "for line in X_train2:\n",
    "    splits = line.split(' ')\n",
    "    sentence.append(splits)\n",
    "for line in ytrain:\n",
    "    labels.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define word2vec model\n",
    "model = gensim.models.Word2Vec(sentence, size=100, sg=1, iter=8)  \n",
    "model.wv.save_word2vec_format('word2Vec' + '.bin', binary=True) \n",
    "wordVec = gensim.models.KeyedVectors.load_word2vec_format('word2Vec.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vec model\n",
    "VECTOR_DIR = 'word2Vec.bin'\n",
    "EMBEDDING_DIM = 100\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(VECTOR_DIR, binary=True)\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items(): \n",
    "    if word in w2v_model:\n",
    "        embedding_matrix[i] = np.asarray(w2v_model[word], dtype='float32')\n",
    "embedding_layer = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=max_length, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 71, 100)           2254000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 71, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 69, 250)           75250     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 23, 250)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5750)              0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 100)               575100    \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 2,904,855\n",
      "Trainable params: 650,855\n",
      "Non-trainable params: 2,254,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#develop CNN model\n",
    "CNN_model = Sequential()\n",
    "CNN_model.add(embedding_layer)\n",
    "CNN_model.add(Dropout(0.2))\n",
    "CNN_model.add(Conv1D(250, 3, padding='valid', activation='relu', strides=1))\n",
    "CNN_model.add(MaxPooling1D(3))\n",
    "CNN_model.add(Flatten())\n",
    "CNN_model.add(Dense(100, activation='relu'))\n",
    "CNN_model.add(Dense(labels_train.shape[1], activation='softmax'))\n",
    "CNN_model.summary()\n",
    "\n",
    "CNN_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 10s 252us/step - loss: 0.4904 - acc: 0.8253\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.4218 - acc: 0.8491\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 7s 178us/step - loss: 0.3821 - acc: 0.8618\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 7s 170us/step - loss: 0.3470 - acc: 0.8736\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 7s 173us/step - loss: 0.3085 - acc: 0.8865\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 7s 169us/step - loss: 0.2721 - acc: 0.8987\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.2365 - acc: 0.9125\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.2106 - acc: 0.9214\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 7s 171us/step - loss: 0.1850 - acc: 0.9302\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 7s 172us/step - loss: 0.1637 - acc: 0.9378\n",
      "CNN running time: 72.33172035217285\n"
     ]
    }
   ],
   "source": [
    "#fit model and calculate time\n",
    "start = time.time()\n",
    "CNN_model.fit(Xtrain, labels_train, epochs=10, verbose=1)\n",
    "end = time.time()\n",
    "CNN_time = end-start\n",
    "print('CNN running time:',CNN_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9794\n",
      "Test Accuracy: 0.8215\n"
     ]
    }
   ],
   "source": [
    "#Evaluate Model\n",
    "sequences = tokenizer.texts_to_sequences(X_test2)\n",
    "Xtest = pad_sequences(sequences, maxlen=max_length)\n",
    "labels_test = to_categorical(np.asarray(y_test))\n",
    "\n",
    "_, acc = CNN_model.evaluate(Xtrain, labels_train, verbose=0)\n",
    "print('Train Accuracy:', acc)\n",
    "# evaluate model on test dataset\n",
    "_, acc = CNN_model.evaluate(Xtest, labels_test, verbose=0)\n",
    "print('Test Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text Generation with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training & testing data\n",
    "Sports_traindata = traindata[traindata['label'] == 2]['text'].tolist()\n",
    "Sports_testdata = testdata[traindata['label'] == 2]['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "train_str = \"\".join(Sports_traindata)\n",
    "tokens = clean_doc2(train_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['washington wizards point guard steve blake miss first month season injuring ankle pickup',\n",
       " 'baseball fans across japan saturday honored ichiro seattle mariners japanese player broke major league baseball record season',\n",
       " 'two years ago ernie els explained possible tiger woods could overtaken world time south african',\n",
       " 'what appeared mismatch took turn competitive second set regrouped',\n",
       " 'serena williams blamed headache poor play upset loss qualifier alina jidkova second round generali ladies']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean text\n",
    "def process_docs(doc):\n",
    "    documents = list()\n",
    "    for line in doc:\n",
    "        line = clean_doc(line)\n",
    "        documents.append(line)\n",
    "    documents.pop()\n",
    "    return documents\n",
    "\n",
    "Sports_train = process_docs(Sports_traindata)\n",
    "Sports_test = process_docs(Sports_testdata)\n",
    "Sports_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 154733\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 15 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)+1):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i - length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "# save sequences to file\n",
    "out_filename = 'sports_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AP Washington Wizards point guard Steve Blake miss first month season injuring ankle pickup fans across'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load doc\n",
    "doc = load_doc('sports_sequences.txt')\n",
    "lines = doc.split('\\n')\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Encode Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16058"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  134,   122,  1471, ..., 16056,   100,  1206],\n",
       "       [  122,  1471,   407, ...,   100,  1206,   502],\n",
       "       [ 1471,   407,   253, ...,  1206,   502,    25],\n",
       "       ...,\n",
       "       [ 3813,  4088,  3185, ...,   431,    19,  1376],\n",
       "       [ 4088,  3185, 16055, ...,    19,  1376,  3554],\n",
       "       [ 3185, 16055,  3208, ...,  1376,  3554,    10]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sequence Inputs and Output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:, :-1], sequences[:, -1]\n",
    "sequences = array(sequences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:, :-1], sequences[:, -1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(vocab_size, seq_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "    model.add(LSTM(100, return_sequences=True))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    # compile network\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    #plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 15, 50)            802900    \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 15, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16058)             1621858   \n",
      "=================================================================\n",
      "Total params: 2,575,658\n",
      "Trainable params: 2,575,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "154733/154733 [==============================] - 104s 672us/step - loss: 8.3153 - acc: 0.0103\n",
      "Epoch 2/25\n",
      "154733/154733 [==============================] - 103s 664us/step - loss: 7.9978 - acc: 0.0124\n",
      "Epoch 3/25\n",
      "154733/154733 [==============================] - 103s 663us/step - loss: 7.8573 - acc: 0.0151\n",
      "Epoch 4/25\n",
      "154733/154733 [==============================] - 102s 662us/step - loss: 7.7336 - acc: 0.0200\n",
      "Epoch 5/25\n",
      "154733/154733 [==============================] - 103s 663us/step - loss: 7.6197 - acc: 0.0240\n",
      "Epoch 6/25\n",
      "154733/154733 [==============================] - 102s 657us/step - loss: 7.5030 - acc: 0.0272\n",
      "Epoch 7/25\n",
      "154733/154733 [==============================] - 102s 658us/step - loss: 7.3885 - acc: 0.0307\n",
      "Epoch 8/25\n",
      "154733/154733 [==============================] - 103s 665us/step - loss: 7.2799 - acc: 0.0351\n",
      "Epoch 9/25\n",
      "154733/154733 [==============================] - 102s 661us/step - loss: 7.1722 - acc: 0.0393\n",
      "Epoch 10/25\n",
      "154733/154733 [==============================] - 102s 662us/step - loss: 7.0675 - acc: 0.0434\n",
      "Epoch 11/25\n",
      "154733/154733 [==============================] - 102s 659us/step - loss: 6.9675 - acc: 0.0475\n",
      "Epoch 12/25\n",
      "154733/154733 [==============================] - 102s 659us/step - loss: 6.8682 - acc: 0.0511\n",
      "Epoch 13/25\n",
      "154733/154733 [==============================] - 102s 659us/step - loss: 6.7718 - acc: 0.0548\n",
      "Epoch 14/25\n",
      "154733/154733 [==============================] - 102s 661us/step - loss: 6.6784 - acc: 0.0581\n",
      "Epoch 15/25\n",
      "154733/154733 [==============================] - 102s 657us/step - loss: 6.5891 - acc: 0.0614\n",
      "Epoch 16/25\n",
      "154733/154733 [==============================] - 102s 660us/step - loss: 6.5050 - acc: 0.0647\n",
      "Epoch 17/25\n",
      "154733/154733 [==============================] - 102s 659us/step - loss: 6.4236 - acc: 0.0677\n",
      "Epoch 18/25\n",
      "154733/154733 [==============================] - 101s 655us/step - loss: 6.3456 - acc: 0.0706\n",
      "Epoch 19/25\n",
      "154733/154733 [==============================] - 101s 654us/step - loss: 6.2698 - acc: 0.0735\n",
      "Epoch 20/25\n",
      "154733/154733 [==============================] - 101s 654us/step - loss: 6.1982 - acc: 0.0758\n",
      "Epoch 21/25\n",
      "154733/154733 [==============================] - 101s 654us/step - loss: 6.1278 - acc: 0.0785\n",
      "Epoch 22/25\n",
      "154733/154733 [==============================] - 101s 650us/step - loss: 6.0620 - acc: 0.0811\n",
      "Epoch 23/25\n",
      "154733/154733 [==============================] - 101s 655us/step - loss: 5.9989 - acc: 0.0844\n",
      "Epoch 24/25\n",
      "154733/154733 [==============================] - 101s 652us/step - loss: 5.9374 - acc: 0.0872\n",
      "Epoch 25/25\n",
      "154733/154733 [==============================] - 100s 649us/step - loss: 5.8779 - acc: 0.0903\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = define_model(vocab_size, seq_length)\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=25)\n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned text sequences\n",
    "in_filename = 'sports_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = len(lines[0].split()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wednesday outstanding match FL The Tampa Bay Lightning captain Dave Andreychuk In accordance team financial terms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "seed_text = lines[randint(0, len(lines))]\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new york yankees win new york patriots abrupt back quarterback john brown hit homer lead\n"
     ]
    }
   ],
   "source": [
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 15)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the world cup qualifier retief schumacher hit homer lead new york yankees beat new york',\n",
       " 'ruled return formula one team players said win world cup qualifier retief schumacher hit homer',\n",
       " 'another olympic committee said said regarding john gibbs said statement victor drug disclose victor conte',\n",
       " 'new york yankees marathon new york yankees beat city the world cup qualifier bode sharapova',\n",
       " 'season torn room called new york yankees interconference marathon world cup qualifier retief schumacher homered']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = list()\n",
    "for i in range(100):\n",
    "    generated = generate_seq(model, tokenizer, seq_length, lines[randint(0, len(lines))], 15)\n",
    "    samples.append(generated)\n",
    "samples[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Use samples to test NB and CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB-tfidf accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "sports_y = [2 for i in range(100)]\n",
    "sports_y = np.array(sports_y)\n",
    "sports_vector=tfidf.transform(samples)\n",
    "prediction_sports=clf.predict(sports_vector)\n",
    "accuracy = accuracy_score(sports_y, prediction_sports)\n",
    "print('NB-tfidf accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN-Word2vec Accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "#Evaluate Model\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "sportstest = pad_sequences(sequences, maxlen=max_length)\n",
    "yadd = np.concatenate((sports_y,y_test),axis=0)\n",
    "sportslabels = to_categorical(np.asarray(yadd))[0:100]\n",
    "\n",
    "_, acc = CNN_model.evaluate(sportstest, sportslabels, verbose=0)\n",
    "print('CNN-Word2vec Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
